{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8909a38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: /home/npkamath/553Project/553ProjectGraphKernels/data/raw/crystal_graphs_dataset.pkl\n",
      "✅ Success: File loaded!\n",
      "\n",
      "Dataset contains 132 graphs and 132 metadata entries.\n",
      "\n",
      "--- GRAPH 0 STRUCTURE ---\n",
      "Object Type: <class 'networkx.classes.graph.Graph'>\n",
      "1. .nodes type: <class 'networkx.classes.reportviews.NodeView'>\n",
      "5. .edges type: <class 'networkx.classes.reportviews.EdgeView'>\n",
      "6. Sample Edge: (0, 1)\n",
      "\n",
      "--- METADATA SAMPLE ---\n",
      "{'crystal_type': 'sc', 'noise_level': 0.0, 'sample_idx': 0, 'n_nodes': 3375, 'n_edges': 10125, 'system_size': 15, 'scale_factor': 1.0, 'ls': (4, 5, 6, 8, 10, 12), 'weight_threshold': 0.05, 'cutoff': 1.8}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. SETUP PATHS\n",
    "# Use pathlib to safely find the file relative to the 'notebooks' folder\n",
    "file_path = Path('..') / 'data' / 'raw' / 'crystal_graphs_dataset.pkl'\n",
    "print(f\"Attempting to load: {file_path.resolve()}\")\n",
    "\n",
    "# 2. DEFINE DUMMY CLASSES (The \"Safety Net\")\n",
    "# If the pickle file looks for 'CrystalGraphDataset' or 'Graph' and can't find them,\n",
    "# it will use this Placeholder class instead of crashing.\n",
    "class Placeholder:\n",
    "    def __init__(self, *args, **kwargs): pass\n",
    "    def __setstate__(self, state): self.__dict__ = state\n",
    "\n",
    "# Inject placeholders into the main namespace\n",
    "if not hasattr(sys.modules['__main__'], 'CrystalGraphDataset'):\n",
    "    setattr(sys.modules['__main__'], 'CrystalGraphDataset', Placeholder)\n",
    "if not hasattr(sys.modules['__main__'], 'Graph'):\n",
    "    setattr(sys.modules['__main__'], 'Graph', Placeholder)\n",
    "\n",
    "# 3. LOAD THE DATA\n",
    "try:\n",
    "    with open(file_path, 'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    print(\"✅ Success: File loaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading file: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 4. INSPECT THE STRUCTURE\n",
    "# We need to know exactly how nodes and edges are stored to write the next script.\n",
    "if isinstance(loaded_data, dict):\n",
    "    graphs = loaded_data.get('graphs', [])\n",
    "    metadata = loaded_data.get('metadata', [])\n",
    "    print(f\"\\nDataset contains {len(graphs)} graphs and {len(metadata)} metadata entries.\")\n",
    "    \n",
    "    if len(graphs) > 0:\n",
    "        g0 = graphs[0]\n",
    "        print(f\"\\n--- GRAPH 0 STRUCTURE ---\")\n",
    "        print(f\"Object Type: {type(g0)}\")\n",
    "        \n",
    "        # Check Node Features (CRITICAL for Discretization)\n",
    "        if hasattr(g0, 'nodes'):\n",
    "            nodes = g0.nodes\n",
    "            print(f\"1. .nodes type: {type(nodes)}\")\n",
    "            \n",
    "            # If list, print first element\n",
    "            if isinstance(nodes, list) and len(nodes) > 0:\n",
    "                feat = nodes[0]\n",
    "                print(f\"2. Node[0] Data: {feat}\")\n",
    "                print(f\"3. Node[0] Type: {type(feat)}\")\n",
    "                if isinstance(feat, np.ndarray):\n",
    "                    print(f\"4. Feature shape: {feat.shape}\")\n",
    "            # If dict, print first key-value\n",
    "            elif isinstance(nodes, dict) and len(nodes) > 0:\n",
    "                first_key = list(nodes.keys())[0]\n",
    "                print(f\"2. Node[{first_key}] Data: {nodes[first_key]}\")\n",
    "        else:\n",
    "            print(\"⚠️ Graph object has no '.nodes' attribute.\")\n",
    "\n",
    "        # Check Edges (CRITICAL for Subgraph Extraction)\n",
    "        if hasattr(g0, 'edges'):\n",
    "            edges = g0.edges\n",
    "            print(f\"5. .edges type: {type(edges)}\")\n",
    "            if hasattr(edges, '__iter__') and len(edges) > 0:\n",
    "                print(f\"6. Sample Edge: {list(edges)[0]}\")\n",
    "        elif hasattr(g0, 'adj'):\n",
    "            print(f\"5. Graph uses adjacency list (.adj) instead of .edges\")\n",
    "        else:\n",
    "            print(\"⚠️ Graph object has no '.edges' attribute.\")\n",
    "            \n",
    "        # Check Metadata\n",
    "        if len(metadata) > 0:\n",
    "            print(f\"\\n--- METADATA SAMPLE ---\")\n",
    "            print(metadata[0])\n",
    "else:\n",
    "    print(f\"Unexpected data structure: {type(loaded_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9a6c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Looking for data at: /home/npkamath/553Project/553ProjectGraphKernels/data/raw/crystal_graphs_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from grakel import Graph\n",
    "from grakel.kernels import WeisfeilerLehmanOptimalAssignment\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Path logic: Go up one level from 'notebooks' to find 'data'\n",
    "RAW_DATA_PATH = Path('..') / 'data' / 'raw' / 'crystal_graphs_dataset.pkl'\n",
    "\n",
    "# Preprocessing Constants\n",
    "NOISE_THRESHOLD = 0.31     # If noise > 0.3, label becomes 'Disordered'\n",
    "NEIGHBOR_RADIUS = 1       # 1 = immediate neighbors only\n",
    "N_BINS = 10               # Number of distinct \"colors\" for Minkowski values\n",
    "\n",
    "# Sampling (CRITICAL for performance)\n",
    "# We take 30 random nodes per graph. \n",
    "# 132 graphs * 30 nodes = ~4,000 training samples.\n",
    "SAMPLES_PER_GRAPH = 5    \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model Constants\n",
    "WL_ITERATIONS = 4         # Depth of the WL refinement (h parameter)\n",
    "SVM_C = 1.0               # Regularization strength\n",
    "\n",
    "print(f\"Configuration loaded. Looking for data at: {RAW_DATA_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d01f3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature_vector(node_attributes):\n",
    "    \"\"\"\n",
    "    Manually extracts M4, M5, M6, M8, M10, M12 into a single numpy array.\n",
    "    \"\"\"\n",
    "    # Define the specific keys we want to use as features\n",
    "    # Based on your debug output\n",
    "    feature_keys = ['M4', 'M5', 'M6', 'M8', 'M10', 'M12']\n",
    "    \n",
    "    features = []\n",
    "    for key in feature_keys:\n",
    "        if key in node_attributes:\n",
    "            features.append(node_attributes[key])\n",
    "        else:\n",
    "            # Fallback if a key is missing (shouldn't happen in clean data)\n",
    "            features.append(0.0)\n",
    "            \n",
    "    return np.array(features)\n",
    "\n",
    "def process_graphs(graphs, metadata):\n",
    "    random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # --- 1. COLLECT ALL FEATURES ---\n",
    "    print(\"Step 1: Collecting features from all graphs...\")\n",
    "    all_features = []\n",
    "\n",
    "    for G in graphs:\n",
    "        # Loop through every node and build the vector manually\n",
    "        for _, data in G.nodes(data=True):\n",
    "            vec = get_node_feature_vector(data)\n",
    "            all_features.append(vec)\n",
    "            \n",
    "    all_features = np.array(all_features)\n",
    "    print(f\"   > Collected {len(all_features)} feature vectors.\")\n",
    "    \n",
    "    # --- 2. TRAIN DISCRETIZER ---\n",
    "    print(f\"Step 2: Discretizing into {N_BINS} bins...\")\n",
    "    discretizer = KBinsDiscretizer(n_bins=N_BINS, encode='ordinal', strategy='kmeans')\n",
    "    discretizer.fit(all_features)\n",
    "    \n",
    "    # --- 3. EXTRACT & LABEL SUBGRAPHS ---\n",
    "    print(\"Step 3: Extracting subgraphs...\")\n",
    "    final_subgraphs = []\n",
    "    final_labels = []\n",
    "    \n",
    "    for G, meta in zip(graphs, metadata):\n",
    "        # A. Determine Class Label\n",
    "        if meta['noise_level'] > NOISE_THRESHOLD:\n",
    "            label = 'Disordered'\n",
    "        else:\n",
    "            label = meta['crystal_type']\n",
    "            \n",
    "        # B. Randomly Sample Nodes\n",
    "        all_nodes = list(G.nodes())\n",
    "        if len(all_nodes) == 0: continue\n",
    "            \n",
    "        # Sample nodes to keep dataset size manageable\n",
    "        selected_nodes = random.sample(all_nodes, min(len(all_nodes), SAMPLES_PER_GRAPH))\n",
    "        \n",
    "        for node_id in selected_nodes:\n",
    "            # Extract neighborhood (Ego Graph)\n",
    "            ego = nx.ego_graph(G, node_id, radius=NEIGHBOR_RADIUS)\n",
    "            \n",
    "            # Prepare GraKeL data structures\n",
    "            gk_labels = {}\n",
    "            \n",
    "            # Get features for nodes in this tiny subgraph\n",
    "            node_ids = list(ego.nodes())\n",
    "            \n",
    "            # BUILD VECTORS FOR THE SUBGRAPH\n",
    "            # We use the same helper function here\n",
    "            raw_feats = [get_node_feature_vector(G.nodes[n]) for n in node_ids]\n",
    "            \n",
    "            # Transform to discrete codes\n",
    "            discrete_codes = discretizer.transform(raw_feats).astype(int)\n",
    "            \n",
    "            # Create a string label for GraKeL: e.g. \"5-19-0-2\"\n",
    "            for i, n_id in enumerate(node_ids):\n",
    "                label_str = \"-\".join(map(str, discrete_codes[i]))\n",
    "                gk_labels[n_id] = label_str\n",
    "            \n",
    "            # Convert edges to list of tuples\n",
    "            gk_edges = list(ego.edges())\n",
    "            \n",
    "            # Store\n",
    "            final_subgraphs.append(Graph(gk_edges, node_labels=gk_labels))\n",
    "            final_labels.append(label)\n",
    "\n",
    "    print(f\"   > Done. Created {len(final_subgraphs)} subgraphs.\")\n",
    "    return final_subgraphs, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf770d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickle file...\n",
      "Step 1: Collecting features from all graphs...\n",
      "   > Collected 375375 feature vectors.\n",
      "Step 2: Discretizing into 10 bins...\n",
      "Step 3: Extracting subgraphs...\n",
      "   > Done. Created 660 subgraphs.\n",
      "\n",
      "--- DEBUG INSPECTION ---\n",
      "Total Samples: 660\n",
      "Sample Label: sc\n",
      "Sample Graph Labels (first 3 nodes):\n",
      "{2619: '9-0-3-8-6-8', 2169: '9-0-3-8-6-8', 2409: '9-0-3-8-6-8'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "print(\"Loading pickle file...\")\n",
    "with open(RAW_DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "graphs = data['graphs']\n",
    "metadata = data['metadata']\n",
    "\n",
    "# 2. Run the Processing Pipeline\n",
    "X, y = process_graphs(graphs, metadata)\n",
    "\n",
    "# 3. Quick Debug Inspection\n",
    "print(\"\\n--- DEBUG INSPECTION ---\")\n",
    "print(f\"Total Samples: {len(X)}\")\n",
    "print(f\"Sample Label: {y[0]}\")\n",
    "print(f\"Sample Graph Labels (first 3 nodes):\")\n",
    "first_graph_labels = X[0].get_labels(purpose='dictionary')\n",
    "# Print first few key-value pairs\n",
    "print(dict(list(first_graph_labels.items())[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b026394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING TRAINING ON 660 SUBGRAPHS ---\n",
      "Train size: 528 | Test size: 132\n",
      "Initializing WL-OA Kernel...\n",
      "Computing Training Kernel Matrix (this allows the SVM to see graph similarity)...\n",
      "Computing Test Kernel Matrix...\n",
      "Fitting SVM Classifier...\n",
      "Predicting on Test Set...\n",
      "\n",
      "--- RESULTS ---\n",
      "Accuracy: 0.4848\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Disordered       0.41      1.00      0.59        48\n",
      "         bcc       1.00      0.24      0.38        21\n",
      "         fcc       1.00      0.14      0.25        21\n",
      "         hcp       1.00      0.19      0.32        21\n",
      "          sc       1.00      0.19      0.32        21\n",
      "\n",
      "    accuracy                           0.48       132\n",
      "   macro avg       0.88      0.35      0.37       132\n",
      "weighted avg       0.79      0.48      0.42       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Training and Evaluation\n",
    "\n",
    "print(f\"--- STARTING TRAINING ON {len(X)} SUBGRAPHS ---\")\n",
    "\n",
    "# 1. Split Data \n",
    "# Stratify ensures we have the same % of FCC/BCC in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "print(f\"Train size: {len(X_train)} | Test size: {len(X_test)}\")\n",
    "\n",
    "# 2. Initialize WL-OA Kernel\n",
    "# n_iter=4 means the algorithm looks 4 hops away (neighbors of neighbors...)\n",
    "# normalize=True is CRITICAL for comparison across different subgraph sizes\n",
    "print(\"Initializing WL-OA Kernel...\")\n",
    "gk = WeisfeilerLehmanOptimalAssignment(n_iter=4, normalize=True, n_jobs=-1)\n",
    "\n",
    "# 3. Compute Gram Matrices\n",
    "print(\"Computing Training Kernel Matrix (this allows the SVM to see graph similarity)...\")\n",
    "# This is the most time-consuming step. It compares every training graph to every other training graph.\n",
    "K_train = gk.fit_transform(X_train)\n",
    "\n",
    "print(\"Computing Test Kernel Matrix...\")\n",
    "K_test = gk.transform(X_test)\n",
    "\n",
    "# 4. Train SVM\n",
    "print(\"Fitting SVM Classifier...\")\n",
    "# C=10 allows for a slightly more complex decision boundary (less regularization)\n",
    "# class_weight='balanced' fixes issues if you have fewer 'Disordered' samples than 'FCC'\n",
    "clf = SVC(kernel='precomputed', C=10.0, class_weight='balanced')\n",
    "clf.fit(K_train, y_train)\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "print(\"Predicting on Test Set...\")\n",
    "y_pred = clf.predict(K_test)\n",
    "\n",
    "print(\"\\n--- RESULTS ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b72bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from: /home/npkamath/553Project/553ProjectGraphKernels/data/raw/crystal_graphs_dataset.pkl\n",
      "Processing graphs (this may take 1-2 minutes)...\n",
      "Step 1: Collecting features from all graphs...\n",
      "Step 2: Discretizing features into 20 bins...\n",
      "Step 3: Relabeling and extracting subgraphs...\n",
      "   > Extraction complete. Created 264 subgraphs.\n",
      "✅ Done! You have 264 subgraphs ready for experimentation.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDACFreud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
