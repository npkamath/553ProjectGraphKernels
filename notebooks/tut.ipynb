{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Tutorial: Processing Crystal Simulations for Graph Kernels\n",
    "\n",
    "**Goal:** Convert raw simulation graphs (from `.pkl` files) into **labeled subgraphs** ready for machine learning experiments.\n",
    "\n",
    "### Why do we need this processing step?\n",
    "1.  **Scaling:** We cannot run Graph Kernels on full 3,000-node simulation boxes (it would take terabytes of RAM). Instead, we extract **Ego Graphs** (local neighborhoods) to characterize the structure.\n",
    "2.  **Compatibility:** The Weisfeiler-Lehman (WL) Kernel requires **discrete labels** (like \"Color A\", \"Color B\"). Our data has continuous Minkowski order parameters ($M_4, M_6...$). This pipeline converts those floats into discrete **\"Bins\"**.\n",
    "3.  **Data Hygiene:** We automatically filter out noisy data. If a simulation has `noise > 0.3`, we label it **'Disordered'** to prevent the model from learning incorrect patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- SETUP PATHS ---\n",
    "# Add the project root to the python path so we can import from 'src'\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import our custom library functions\n",
    "from src.config import RAW_DATA_PATH\n",
    "from src.processing import process_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "We load the dataset containing 132 large crystal simulations. Each graph represents one full simulation box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/npkamath/553Project/553ProjectGraphKernels/data/raw/crystal_graphs_dataset.pkl\n",
      "Loaded 132 simulations.\n",
      "Sample Metadata: {'crystal_type': 'sc', 'noise_level': 0.0, 'sample_idx': 0, 'n_nodes': 3375, 'n_edges': 10125, 'system_size': 15, 'scale_factor': 1.0, 'ls': (4, 5, 6, 8, 10, 12), 'weight_threshold': 0.05, 'cutoff': 1.8}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from: {RAW_DATA_PATH}\")\n",
    "with open(RAW_DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "graphs = data['graphs']\n",
    "metadata = data['metadata']\n",
    "\n",
    "print(f\"Loaded {len(graphs)} simulations.\")\n",
    "print(f\"Sample Metadata: {metadata[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Processing Pipeline (`process_graphs`)\n",
    "\n",
    "We now run the core function `process_graphs`. This function performs three critical steps automatically:\n",
    "\n",
    "1.  **Discretization:** It collects all Minkowski features ($M_4 \\dots M_{12}$) from every node and trains a **K-Means model** to group them into **20 discrete bins**. \n",
    "    * *Example:* `[0.12, 0.88]` $\\rightarrow$ `Bin 2` and `Bin 18` $\\rightarrow$ Label `\"2-18\"`.\n",
    "2.  **Relabeling:** It checks the `noise_level` in the metadata. If `noise > 0.3`, the graph is labeled **'Disordered'**, regardless of its original type.\n",
    "3.  **Subgraph Extraction:** It randomly samples **30 nodes** from each graph and extracts their **1-hop neighborhood** (Ego Graph). This creates a dataset of ~4,000 small, manageable graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running processing pipeline (this may take 1-2 minutes)...\n",
      "Step 1: Collecting features from all graphs...\n",
      "Step 2: Discretizing features into 20 bins...\n",
      "Step 3: Relabeling and extracting subgraphs...\n",
      "   > Extraction complete. Created 396 subgraphs.\n",
      "âœ… Success! Generated 396 subgraphs ready for training.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running processing pipeline (this may take 1-2 minutes)...\")\n",
    "\n",
    "# Returns:\n",
    "#   subgraphs: A list of grakel.Graph objects\n",
    "#   labels: A list of strings (e.g., 'FCC', 'BCC', 'Disordered')\n",
    "subgraphs, labels = process_graphs(graphs, metadata)\n",
    "\n",
    "print(f\"âœ… Success! Generated {len(subgraphs)} subgraphs ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting the Data\n",
    "Let's look at one of the subgraphs to understand what the Kernel will see.\n",
    "Notice the **node labels** are strings like `'17-0-5...'`. These are the discretized \"Barcodes\" representing the local physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 0 ---\n",
      "Ground Truth Label: sc\n",
      "Number of nodes in subgraph: 7\n",
      "\n",
      "Node Labels (The 'Barcode' the kernel sees):\n",
      "  Node 2619: 18-0-8-19-11-19\n",
      "  Node 2169: 18-0-8-19-11-19\n",
      "  Node 2409: 18-0-8-19-11-19\n",
      "  Node 2379: 18-0-8-19-11-19\n",
      "  Node 2393: 18-0-8-19-11-19\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first sample\n",
    "sample_idx = 0\n",
    "sample_graph = subgraphs[sample_idx]\n",
    "sample_label = labels[sample_idx]\n",
    "\n",
    "print(f\"--- Sample {sample_idx} ---\")\n",
    "print(f\"Ground Truth Label: {sample_label}\")\n",
    "\n",
    "# Get internal GraKeL representation\n",
    "gk_labels = sample_graph.get_labels(purpose='dictionary')\n",
    "\n",
    "print(f\"Number of nodes in subgraph: {len(gk_labels)}\")\n",
    "print(\"\\nNode Labels (The 'Barcode' the kernel sees):\")\n",
    "for node_id, label in list(gk_labels.items())[:5]:\n",
    "    print(f\"  Node {node_id}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ready for Experiments\n",
    "You can now use `subgraphs` and `labels` in your own experiments. You do **not** need to use the default SVM pipeline.\n",
    "\n",
    "### Example: Custom Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 277 graphs\n",
      "Test Set:     119 graphs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the processed data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    subgraphs, \n",
    "    labels, \n",
    "    test_size=0.3, \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {len(X_train)} graphs\")\n",
    "print(f\"Test Set:     {len(X_test)} graphs\")\n",
    "\n",
    "# NOW YOU CAN INSERT YOUR CUSTOM KERNEL/MODEL CODE HERE\n",
    "# Example:\n",
    "# from grakel.kernels import WeisfeilerLehman\n",
    "# gk = WeisfeilerLehman(n_iter=3)\n",
    "# K_train = gk.fit_transform(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDACFreud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
